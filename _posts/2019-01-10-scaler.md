---
layout: post
title: "[Python] Which scaling technique should I use?"
subtitle: "Features by Scaler"
author: "MK"
comments: true
sitemap :
  changefreq : daily
  priority : 1.0
categories : [Python]
tags: [Modeling Tips, Preprocessing]
---


Before data can be modeled, it must be `scaled`. Scaling makes **it easy to compare and analyze multi-dimensional values**, **avoid overflow or underflow of data**, and set the condition number of the covariance matrix of independent variables **To improve stability and convergence speed during the optimization process.**


especially, Scaling is important for distance-based models such as k-means.

In regression analysis, there is the concept of condition number, which is as follows.

<br><br>

### Condition number in regression analysis
---


The formula representing the relation between the `eigenvalue` and the `condition number` is `c = condeig (A)`.

Returns the vector of condition numbers for the eigenvalues of A. This condition number is the reciprocal of the cosine of the angle between the left eigenvector and the right eigenvector. Let's skip the complicated explanation and look at the core.

The `condition number` of a function is an argument measure of **how much a function can change relative to the ratio of small changes** in an argument.

**_If the number of conditions is large, the solution has a completely different value even if there are some errors. Therefore, if the number of conditions is large, the predicted value using the regression analysis will also increase in error._**

**There are two major cases where the number of conditions increases in regression analysis.**

1) If the scale of the numbers is greatly changed due to the unit difference of the variables, it is solved by 'scaling'.

2) If there are multiple collinearity, that is, large independent variables that are highly correlated, this case can be solved by 'selecting a variable' or reducing the dimension using 'PCA'.

<br><br>

In the following cases, the regression performance may be improved by using the transformed variables using the log function or the square root function.

> When independent variables or dependent variables are severely biased
> If the relationship between the independent variable and the dependent variable is connected by multiplication or division
> When the dependent variable and the predicted value show a nonlinear relationship

That's why most of us deal with **large numeric data, such as amounts of data **.

**_Typically, this data is logarithmically selected and then scaled as a whole before modeling.(in my opinion)_**

<br><br>

#### So Which scaling technique should I use?

<br><br>

### Types of scaling
---
Scikit-Learn offers a wide variety of scalers. Among them are representative techniques.

|   | Type            | Explanation                                                                |
|---|-----------------|----------------------------------------------------------------------------|
| 1 | StandardScaler  | The default scale. Using Mean and Standard Deviation                       |
| 2 | MinMaxScaler    | Scaling so that the maximum / minimum values are 1, 0                      |
| 3 | MaxAbsScaler    | Scaling so that the maximum absolute value and 0 are 1 and 0, respectively |
| 4 | RobustScaler    | Use median and IQR (interquartile range). Minimize the impact of outliers  |

<br>

#### 1. StandardScaler
---
Remove the mean and adjust the data to unit variance. However, if there is an anomaly, the spread of the converted data is very different because it affects the mean and standard deviation.

**Thus, if there is an anomaly, a balanced scale can not be guaranteed.**

```python
from sklearn.preprocessing import StandardScaler
standardScaler = StandardScaler()
print(standardScaler.fit(train_data))
train_data_standardScaled = standardScaler.transform(train_data)
```

<br>

#### 2. MinMaxScaler
---
Readjust the data so that all feature values are between 0 and 1. However, if there is an abnormal value, the converted value can be compressed to a very narrow range.

In other words, MinMaxScaler is also very sensitive to the presence of outliers.

```python
from sklearn.preprocessing import MinMaxScaler
minMaxScaler = MinMaxScaler()
print(minMaxScaler.fit(train_data))
train_data_minMaxScaled = minMaxScaler.transform(train_data)
```

<br>

#### 3. MaxAbsScaler
---
Let the absolute value be mapped between 0 and 1. Ie, that is readjusted between -1 and 1. Feature datasets that consist only of positive data behave similarly to MinMaxScaler and can be sensitive to large outliers.

```python
from sklearn.preprocessing import MaxAbsScaler
maxAbsScaler = MaxAbsScaler()
print(maxAbsScaler.fit(train_data))
train_data_maxAbsScaled = maxAbsScaler.transform(train_data)
```
<br>

#### 4. RobustScaler
---
It is a technique that minimizes the influence of outliers. The median and IQR(interquartile range) are used, so it can be confirmed that the same **value is distributed more widely after standardization** compared with StandardScaler.


**_IQR = Q3 - Q1 : That is, 25 and 75 percentile values._**

The results of standardization of data including outliers are as follows.
![img_area](/img/posting/2019-01-10-001-robustscaler.png)


```python
from sklearn.preprocessing import RobustScaler
robustScaler = RobustScaler()
print(robustScaler.fit(train_data))
train_data_robustScaled = robustScaler.transform(train_data)
```

In conclusion, outlier removal should precede all scaler processing.
It is also advisable to apply an appropriate scaler according to the distribution characteristics of the data.

<br><br>

### Conversion results by data distribution
---
![img_area](/img/posting/2019-01-10-001-ex1.PNG)
![img_area](/img/posting/2019-01-10-001-ex2.PNG)
![img_area](/img/posting/2019-01-10-001-ex3.PNG)
![img_area](/img/posting/2019-01-10-001-ex4.PNG)
![img_area](/img/posting/2019-01-10-001-ex5.PNG)

If you look at the conversion distribution, the converted results of `StandardScaler` and` RobustScaler` are mostly returned as standardized data distribution of similar type.

Data concentrated on a `MinMaxScaler` specific value will have a larger scale change value by one standard deviation than a data distribution not having a specific value. The distribution of data with one side deviation results in that the range value is adjusted while the shape is almost maintained.

`MaxAbsScaler` is similar to MinMaxScaler but maintains a symmetric distribution according to negative and positive values.

And from the last image, ** Outliers in most scaling schemes show that they are a factor that hampers conversion. **

Note that **it is important to make similar sizes for each feature when scaling, but it is not necessary to make all feature distributions the same**.

It is noted that depending on the characteristics, some items maintain the distribution of the original data. For example, when a 'feature' is concentrated in almost one place and the distribution is made the same by standardizing it, small unit changes can be reflected as a large difference.

<br><br>

### **Reference**
---
- https://datascienceschool.net/view-notebook/afb99de8cc0d407ba32079590b25180d/
- https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html
- http://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221293217463&categoryNo=49&parentCategoryNo=0&viewDate=&currentPage=1&postListTopCurrentPage=1&from=search

<br>
