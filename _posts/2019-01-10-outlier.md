---
layout: post
title: "[Python] How to remove outlier"
subtitle: 
author: "MK"
comments: true
sitemap :
  changefreq : daily
  priority : 1.0
categories : [Python]
tags: [Preprocessing]
---


There is an assumption in the standardization conversion **"there should be no outliers"**. The formula for converting to the standard normal distribution is `z = (x - average) / standard deviation`, and **mean is very sensitive to outliers and singular values.**


You can find out about the need to remove scale outers from the link below.

[Previous posting] Scaler types and features [Go to check](https://mkjjo.github.io/2019/01/10/scaler.html)


Generally, there is `6 sigma`, that is, a method to remove the value corresponding to the plus & minus three standard deviation, as an outlier.

And the other side, there is a method of using `RoubustScaler` when performing modeling scaling processing.
In addition, the `sklearn package` can solve the problem.

<br><br>

### 1. Using 'Roubust' instead of 'Standard'
---


This technique minimizes the influence of outliers. Because of using the median and IQR(interquartile range), it can be confirmed that **the same value is distributed more widely after standardization** compared with StandardScaler.

**_IQR = Q3 - Q1 : That is, 25 and 75 percentile values._**

The results of standardization of data including outliers are as follows.
![img_area](/img/posting/2019-01-10-001-robustscaler.png)


```python
from sklearn.preprocessing import RobustScaler
robustScaler = RobustScaler()
print(robustScaler.fit(train_data))
train_data_robustScaled = robustScaler.transform(train_data)
```

<br><br>

### 2. Using the sklearn package
---

There are two broad categories and four methodologies.

**_First, novelty detection_**
When a new value comes in, How to distinguish whether the value is appropriate for an existing distribution


**_Second, outlier detection_**
How to determine the outliers among the current values.

The types are as follows.


|   | types                         | df    |
|---|------------------------------|---------|
| 1 | One-class SVM                | Novelty |
| 2 | Fitting an elliptic envelope | Outlier |
| 3 | Isolation Forest             | Outlier |
| 4 | Local Outlier Factor         | Outlier |

<br>

#### One-class SVM
As the methodology for Novelty, if the outline of the initial observation distribution is drawn in a new dimension space and the additional observations are within the bounded space from the boundary, they are regarded as the same group as the initial observations or otherwise abnormal data.

![img_area](/img/posting/2019-01-10-002-oneclasssvm.PNG){: .post-img}

<br>

#### Fitting an elliptic envelope
This methodology requires assumptions about data distribution. The inlier position and covariance can be estimated by assuming that the inlier data is a Gaussian distribution. The Mahalanobis distances thus obtained are used to derive the alienation measure.

Scikit-learn provides object covariance. By applying a strong covariance estimate to the data, the outlier can be eliminated by matching the ellipse to the central data point and ignoring points outside of the center mode.

![img_area](/img/posting/2019-01-10-002-fitting.PNG){: .post-img}

<br>

#### Isolation Forest

t is an outlier removal method that works efficiently in multidimensional datasets.
The 'Isolation Forest' consists of observations that randomly separate the 'MinMax' value of the randomly selected 'Feature'.

Since the recursive partition can be represented by a tree structure, the number of divisions required to separate the samples is equal to the path length from the root node to the terminating node.

This path length averaged over these random trees is a measure of normality and decision.


The randomization of the outliers results in a significantly shorter path. Therefore, it is more likely to be an outlier when generating a shorter path length for a particular sample.

![img_area](/img/posting/2019-01-10-002-isolationforest.PNG){: .post-img}


```python
# To use the Isolation Forest method, declare it as a variable.
clf = IsolationForest(max_samples=1000, random_state=1)

# Use the fit function to learn the dataset. race_for_out is the name of the dataframe.
clf.fit(race_for_out)

# Use the predict function to determine outliers. Data of series type consisting of 0 and 1 is output.
y_pred_outliers = clf.predict(race_for_out)

# Attach to the original dataframe. Since 0 is an outlier, removing a zero leaves a dataframe with outliers removed.
out = pd.DataFrame(y_pred_outliers)
out = out.rename(columns={0: "out"})
race_an1 = pd.concat([race_for_out, out], 1)
```

<br>

#### Local Outlier Factor

This method is also effective for removing outliers of multidimensional data.

The algorithm calculates a score (local outlier coefficient) that reflects an abnormal degree of observation.

It measures the local density variance for a neighbor of a given data point and then detects a sample that is much less dense than its neighbors.

In practice, the local density is obtained from the closest k neighbors. Normal instances are expected to have local densities similar to neighbors, but unusual data are expected to have much smaller local densities.

The strength of the LOF algorithm is that it considers both the local and global attributes of the data set. Performance is also excellent for datasets with different base densities of abnormal samples. It is not how much the sample is isolated, but how much it is isolated to its neighbors.

![img_area](/img/posting/2019-01-10-002-lof.PNG){: .post-img}






<br><br>

### **Reference**
---
https://tariat.tistory.com/29
https://scikit-learn.org/stable/modules/outlier_detection.html
<br>
